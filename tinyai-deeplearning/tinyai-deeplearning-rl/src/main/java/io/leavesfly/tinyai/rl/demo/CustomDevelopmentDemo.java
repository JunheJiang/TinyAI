package io.leavesfly.tinyai.rl.demo;

import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.ndarr.NdArray;
import io.leavesfly.tinyai.ndarr.Shape;
import io.leavesfly.tinyai.rl.*;
import io.leavesfly.tinyai.rl.agent.BanditAgent;

import java.util.HashMap;
import java.util.Map;
import java.util.Random;

/**
 * è‡ªå®šä¹‰å¼€å‘å®Œæ•´æ¼”ç¤º
 * 
 * <p>æœ¬æ¼”ç¤ºå±•ç¤ºå¦‚ä½•æ‰©å±•TinyAI RLæ¡†æ¶:
 * <ul>
 *   <li>è‡ªå®šä¹‰ç¯å¢ƒ: åˆ›å»ºæ–°çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ</li>
 *   <li>è‡ªå®šä¹‰æ™ºèƒ½ä½“: å®ç°æ–°çš„å­¦ä¹ ç®—æ³•</li>
 *   <li>é›†æˆä½¿ç”¨: å°†è‡ªå®šä¹‰ç»„ä»¶æ•´åˆåˆ°æ¡†æ¶ä¸­</li>
 * </ul>
 * 
 * <p><b>åœºæ™¯</b>: ç®€å•çš„è¿·å®«å¯»å®æ¸¸æˆ
 * <ul>
 *   <li>5x5ç½‘æ ¼è¿·å®«</li>
 *   <li>æ™ºèƒ½ä½“éœ€è¦æ‰¾åˆ°å®è—</li>
 *   <li>é¿å¼€é™·é˜±</li>
 * </ul>
 * 
 * <p><b>è¿è¡Œæ–¹å¼:</b>
 * <pre>
 * mvn exec:java -Dexec.mainClass="io.leavesfly.tinyai.rl.demo.CustomDevelopmentDemo" \
 *   -pl tinyai-deeplearning-rl
 * </pre>
 * 
 * @author TinyAI Team
 */
public class CustomDevelopmentDemo {

    public static void main(String[] args) {
        System.out.println("==========================================");
        System.out.println("       è‡ªå®šä¹‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸æ™ºèƒ½ä½“æ¼”ç¤º       ");
        System.out.println("==========================================\n");

        demonstrateCustomEnvironment();
        demonstrateCustomAgent();
        demonstrateIntegration();

        System.out.println("\n==========================================");
        System.out.println("          è‡ªå®šä¹‰å¼€å‘æ¼”ç¤ºå®Œæˆ!             ");
        System.out.println("==========================================");
    }

    /**
     * æ¼”ç¤ºè‡ªå®šä¹‰ç¯å¢ƒ
     */
    private static void demonstrateCustomEnvironment() {
        System.out.println("ã€æ­¥éª¤1: åˆ›å»ºè‡ªå®šä¹‰ç¯å¢ƒã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        System.out.println("åœºæ™¯: 5x5è¿·å®«å¯»å®\n");

        TreasureMazeEnvironment env = new TreasureMazeEnvironment();
        
        System.out.println("è¿·å®«å¸ƒå±€:");
        env.render();
        
        System.out.println("\nç¯å¢ƒç‰¹æ€§:");
        System.out.println("  çŠ¶æ€ç»´åº¦: " + env.getStateDim() + " (xåæ ‡ + yåæ ‡)");
        System.out.println("  åŠ¨ä½œç»´åº¦: " + env.getActionDim() + " (ä¸Šä¸‹å·¦å³)");
        
        System.out.println("\nå¥–åŠ±è®¾è®¡:");
        System.out.println("  â€¢ æ‰¾åˆ°å®è—: +10");
        System.out.println("  â€¢ è¸©åˆ°é™·é˜±: -5");
        System.out.println("  â€¢ æ¯æ­¥ç§»åŠ¨: -0.1");
        System.out.println("  â€¢ åˆ°è¾¾è¾¹ç•Œ: -1\n");
    }

    /**
     * æ¼”ç¤ºè‡ªå®šä¹‰æ™ºèƒ½ä½“
     */
    private static void demonstrateCustomAgent() {
        System.out.println("ã€æ­¥éª¤2: åˆ›å»ºè‡ªå®šä¹‰æ™ºèƒ½ä½“ã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        System.out.println("ç®—æ³•: ç®€å•Q-Learning (è¡¨æ ¼å‹)\n");

        QLearningAgent agent = new QLearningAgent(
            "Q-Learningæ¢é™©å®¶",
            2,  // çŠ¶æ€ç»´åº¦
            4,  // åŠ¨ä½œç»´åº¦  
            0.1f,  // å­¦ä¹ ç‡
            0.1f,  // æ¢ç´¢ç‡
            0.9f   // æŠ˜æ‰£å› å­
        );

        System.out.println("æ™ºèƒ½ä½“ç‰¹æ€§:");
        System.out.println("  ç®—æ³•ç±»å‹: Q-Learning");
        System.out.println("  Qè¡¨ç»“æ„: 25ä¸ªçŠ¶æ€ Ã— 4ä¸ªåŠ¨ä½œ");
        System.out.println("  å­¦ä¹ æ–¹å¼: æ—¶åºå·®åˆ†å­¦ä¹ ");
        System.out.println("  ç­–ç•¥: Îµ-è´ªå¿ƒ\n");
    }

    /**
     * æ¼”ç¤ºé›†æˆä½¿ç”¨
     */
    private static void demonstrateIntegration() {
        System.out.println("ã€æ­¥éª¤3: è®­ç»ƒè‡ªå®šä¹‰æ™ºèƒ½ä½“ã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

        TreasureMazeEnvironment env = new TreasureMazeEnvironment();
        QLearningAgent agent = new QLearningAgent("æ¢é™©å®¶", 2, 4, 0.1f, 0.1f, 0.9f);

        System.out.println("è®­ç»ƒè¿›åº¦:");
        System.out.println("å›åˆ | æ­¥æ•° | å¥–åŠ± | ç»“æœ");
        System.out.println("-----|------|------|----------");

        int maxEpisodes = 100;
        int successCount = 0;

        for (int episode = 0; episode < maxEpisodes; episode++) {
            Variable state = env.reset();
            float episodeReward = 0;
            int steps = 0;

            while (!env.isDone() && steps < 50) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = env.step(action);

                Experience experience = new Experience(
                    state, action, result.getReward(),
                    result.getNextState(), result.isDone(), steps
                );
                agent.learn(experience);

                state = result.getNextState();
                episodeReward += result.getReward();
                steps++;
            }

            if (episodeReward > 5) {
                successCount++;
            }

            if (episode < 10 || (episode + 1) % 20 == 0) {
                String result = episodeReward > 5 ? "æ‰¾åˆ°å®è—âœ“" : "æœªæ‰¾åˆ°";
                System.out.printf(" %3d | %3d  | %5.1f | %s\n",
                    episode + 1, steps, episodeReward, result);
            }
        }

        System.out.println("\nè®­ç»ƒç»“æœ:");
        System.out.println("  æˆåŠŸæ¬¡æ•°: " + successCount + "/" + maxEpisodes);
        System.out.println("  æˆåŠŸç‡: " + String.format("%.1f%%", (float) successCount / maxEpisodes * 100));

        System.out.println("\nå­¦åˆ°çš„ç­–ç•¥(æœ€åä¸€æ¬¡):");
        env.reset();
        env.render();

        System.out.println("\nã€å…³é”®ä»£ç æ¨¡æ¿ã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        
        System.out.println("\n1. è‡ªå®šä¹‰ç¯å¢ƒæ¨¡æ¿:");
        System.out.println("```java");
        System.out.println("public class CustomEnvironment extends Environment {");
        System.out.println("    public CustomEnvironment() {");
        System.out.println("        super(stateDim, actionDim, maxSteps);");
        System.out.println("    }");
        System.out.println("    ");
        System.out.println("    @Override");
        System.out.println("    public Variable reset() {");
        System.out.println("        // é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€");
        System.out.println("        return initialState;");
        System.out.println("    }");
        System.out.println("    ");
        System.out.println("    @Override");
        System.out.println("    public StepResult step(Variable action) {");
        System.out.println("        // æ‰§è¡ŒåŠ¨ä½œ,è®¡ç®—ä¸‹ä¸€çŠ¶æ€å’Œå¥–åŠ±");
        System.out.println("        return new StepResult(nextState, reward, done, info);");
        System.out.println("    }");
        System.out.println("}");
        System.out.println("```");

        System.out.println("\n2. è‡ªå®šä¹‰æ™ºèƒ½ä½“æ¨¡æ¿:");
        System.out.println("```java");
        System.out.println("public class CustomAgent extends Agent {");
        System.out.println("    @Override");
        System.out.println("    public Variable selectAction(Variable state) {");
        System.out.println("        // å®ç°åŠ¨ä½œé€‰æ‹©é€»è¾‘");
        System.out.println("        return selectedAction;");
        System.out.println("    }");
        System.out.println("    ");
        System.out.println("    @Override");
        System.out.println("    public void learn(Experience experience) {");
        System.out.println("        // å®ç°å­¦ä¹ æ›´æ–°é€»è¾‘");
        System.out.println("    }");
        System.out.println("}");
        System.out.println("```");

        System.out.println("\nğŸ’¡ æ‰©å±•å»ºè®®:");
        System.out.println("  â€¢ å‚è€ƒç°æœ‰ç¯å¢ƒå®ç°: CartPoleEnvironment, GridWorldEnvironment");
        System.out.println("  â€¢ å‚è€ƒç°æœ‰æ™ºèƒ½ä½“å®ç°: DQNAgent, REINFORCEAgent");
        System.out.println("  â€¢ éµå¾ªæ¥å£è§„èŒƒ,ç¡®ä¿å…¼å®¹æ€§");
        System.out.println("  â€¢ æ·»åŠ è¯¦ç»†æ³¨é‡Š,ä¾¿äºç»´æŠ¤");
    }

    /**
     * è‡ªå®šä¹‰ç¯å¢ƒ: å¯»å®è¿·å®«
     */
    private static class TreasureMazeEnvironment extends Environment {
        private static final int SIZE = 5;
        private int[] agentPos;
        private int[] treasurePos;
        private int[] trapPos;
        private Random random;

        public TreasureMazeEnvironment() {
            super(2, 4, 50);
            this.random = new Random();
            this.treasurePos = new int[]{4, 4};  // å®è—åœ¨å³ä¸‹è§’
            this.trapPos = new int[]{2, 2};      // é™·é˜±åœ¨ä¸­é—´
        }

        @Override
        public Variable reset() {
            this.agentPos = new int[]{0, 0};  // èµ·ç‚¹åœ¨å·¦ä¸Šè§’
            this.done = false;
            this.currentStep = 0;
            return new Variable(NdArray.of(new float[]{agentPos[0], agentPos[1]}));
        }

        @Override
        public StepResult step(Variable action) {
            int actionValue = (int) action.getValue().getNumber().floatValue();
            
            // ç§»åŠ¨: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³
            int[] newPos = agentPos.clone();
            switch (actionValue) {
                case 0: newPos[1] = Math.max(0, newPos[1] - 1); break;  // ä¸Š
                case 1: newPos[1] = Math.min(SIZE - 1, newPos[1] + 1); break;  // ä¸‹
                case 2: newPos[0] = Math.max(0, newPos[0] - 1); break;  // å·¦
                case 3: newPos[0] = Math.min(SIZE - 1, newPos[0] + 1); break;  // å³
            }

            float reward = -0.1f;  // ç§»åŠ¨æƒ©ç½š
            boolean reachedGoal = false;

            // æ£€æŸ¥è¾¹ç•Œæƒ©ç½š
            if (newPos[0] == agentPos[0] && newPos[1] == agentPos[1]) {
                reward = -1.0f;  // æ’å¢™
            }

            agentPos = newPos;

            // æ£€æŸ¥å®è—
            if (agentPos[0] == treasurePos[0] && agentPos[1] == treasurePos[1]) {
                reward = 10.0f;
                done = true;
                reachedGoal = true;
            }

            // æ£€æŸ¥é™·é˜±
            if (agentPos[0] == trapPos[0] && agentPos[1] == trapPos[1]) {
                reward = -5.0f;
                done = true;
            }

            currentStep++;
            if (currentStep >= maxSteps) {
                done = true;
            }

            Variable nextState = new Variable(NdArray.of(new float[]{agentPos[0], agentPos[1]}));
            
            Map<String, Object> info = new HashMap<>();
            info.put("reachedGoal", reachedGoal);

            return new StepResult(nextState, reward, done, info);
        }

        @Override
        public Variable sampleAction() {
            return new Variable(NdArray.of(random.nextInt(4)));
        }

        @Override
        public boolean isValidAction(Variable action) {
            int actionValue = (int) action.getValue().getNumber().floatValue();
            return actionValue >= 0 && actionValue < 4;
        }

        @Override
        public void render() {
            System.out.println("  å›¾ä¾‹: A=æ™ºèƒ½ä½“, T=å®è—, X=é™·é˜±, .=ç©ºåœ°");
            for (int y = 0; y < SIZE; y++) {
                System.out.print("  ");
                for (int x = 0; x < SIZE; x++) {
                    if (x == agentPos[0] && y == agentPos[1]) {
                        System.out.print("A ");
                    } else if (x == treasurePos[0] && y == treasurePos[1]) {
                        System.out.print("T ");
                    } else if (x == trapPos[0] && y == trapPos[1]) {
                        System.out.print("X ");
                    } else {
                        System.out.print(". ");
                    }
                }
                System.out.println();
            }
        }
    }

    /**
     * è‡ªå®šä¹‰æ™ºèƒ½ä½“: Q-Learning
     */
    private static class QLearningAgent extends Agent {
        private Map<String, float[]> qTable;
        private Random random;

        public QLearningAgent(String name, int stateDim, int actionDim,
                            float learningRate, float epsilon, float gamma) {
            super(name, stateDim, actionDim, learningRate, epsilon, gamma);
            this.qTable = new HashMap<>();
            this.random = new Random();
        }

        @Override
        public Variable selectAction(Variable state) {
            String stateKey = getStateKey(state);
            
            if (!qTable.containsKey(stateKey)) {
                qTable.put(stateKey, new float[actionDim]);
            }

            // Îµ-è´ªå¿ƒç­–ç•¥
            if (training && random.nextFloat() < epsilon) {
                return new Variable(NdArray.of(random.nextInt(actionDim)));
            } else {
                float[] qValues = qTable.get(stateKey);
                int bestAction = 0;
                for (int i = 1; i < actionDim; i++) {
                    if (qValues[i] > qValues[bestAction]) {
                        bestAction = i;
                    }
                }
                return new Variable(NdArray.of(bestAction));
            }
        }

        @Override
        public void learn(Experience experience) {
            String stateKey = getStateKey(experience.getState());
            String nextStateKey = getStateKey(experience.getNextState());
            
            if (!qTable.containsKey(stateKey)) {
                qTable.put(stateKey, new float[actionDim]);
            }
            if (!qTable.containsKey(nextStateKey)) {
                qTable.put(nextStateKey, new float[actionDim]);
            }

            int action = (int) experience.getAction().getValue().getNumber().floatValue();
            float reward = experience.getReward();
            
            // Q-Learningæ›´æ–°: Q(s,a) = Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
            float[] qValues = qTable.get(stateKey);
            float[] nextQValues = qTable.get(nextStateKey);
            
            float maxNextQ = experience.isDone() ? 0 : getMaxQValue(nextQValues);
            float tdTarget = reward + gamma * maxNextQ;
            float tdError = tdTarget - qValues[action];
            
            qValues[action] += learningRate * tdError;
            trainingStep++;
        }

        @Override
        public void storeExperience(Experience experience) {
            // Q-Learningä¸éœ€è¦ç»éªŒå›æ”¾
        }

        @Override
        public void learnBatch(Experience[] experiences) {
            for (Experience exp : experiences) {
                learn(exp);
            }
        }

        @Override
        public void loadModel(String modelPath) {
            // Q-Learningä¸æ”¯æŒæ¨¡å‹åŠ è½½
            throw new UnsupportedOperationException("Q-Learningä¸æ”¯æŒæ¨¡å‹åŠ è½½");
        }

        @Override
        public void saveModel(String modelPath) {
            // Q-Learningä¸æ”¯æŒæ¨¡å‹ä¿å­˜
            throw new UnsupportedOperationException("Q-Learningä¸æ”¯æŒæ¨¡å‹ä¿å­˜");
        }

        private String getStateKey(Variable state) {
            float[] data = state.getValue().getArray();
            return String.format("%d,%d", (int) data[0], (int) data[1]);
        }

        private float getMaxQValue(float[] qValues) {
            float max = qValues[0];
            for (int i = 1; i < qValues.length; i++) {
                if (qValues[i] > max) {
                    max = qValues[i];
                }
            }
            return max;
        }
    }
}
