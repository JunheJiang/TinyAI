package io.leavesfly.tinyai.rl.demo;

import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.rl.Experience;
import io.leavesfly.tinyai.rl.agent.DQNAgent;
import io.leavesfly.tinyai.rl.environment.CartPoleEnvironment;
import io.leavesfly.tinyai.rl.Environment;

/**
 * DQNç®—æ³•å®Œæ•´æ¼”ç¤º - CartPoleç¯å¢ƒ
 * 
 * <p>æœ¬æ¼”ç¤ºå±•ç¤ºå¦‚ä½•ä½¿ç”¨DQN(Deep Q-Network)ç®—æ³•è§£å†³CartPoleå€’ç«‹æ‘†é—®é¢˜:
 * <ul>
 *   <li>DQNæ ¸å¿ƒç»„ä»¶: Qç½‘ç»œã€ç›®æ ‡ç½‘ç»œã€ç»éªŒå›æ”¾</li>
 *   <li>è®­ç»ƒè¿‡ç¨‹: æ¢ç´¢ç‡è¡°å‡ã€æ‰¹é‡å­¦ä¹ ã€ç½‘ç»œæ›´æ–°</li>
 *   <li>æ€§èƒ½è¯„ä¼°: å¹³å‡å¥–åŠ±ã€æˆåŠŸç‡ã€å­¦ä¹ æ›²çº¿</li>
 * </ul>
 * 
 * <p><b>è¿è¡Œæ–¹å¼:</b>
 * <pre>
 * mvn exec:java -Dexec.mainClass="io.leavesfly.tinyai.rl.demo.DQNCartPoleDemo" \
 *   -pl tinyai-deeplearning-rl
 * </pre>
 * 
 * @author TinyAI Team
 */
public class DQNCartPoleDemo {

    // è®­ç»ƒè¶…å‚æ•°
    private static final int MAX_EPISODES = 300;
    private static final int EVAL_INTERVAL = 50;
    private static final int EVAL_EPISODES = 10;
    
    public static void main(String[] args) {
        System.out.println("==========================================");
        System.out.println("        DQNç®—æ³•å®Œæ•´æ¼”ç¤º - CartPole        ");
        System.out.println("==========================================\n");

        demonstrateProblem();
        DQNAgent agent = setupDQNAgent();
        CartPoleEnvironment env = new CartPoleEnvironment(500);
        
        trainAgent(agent, env);
        evaluateAgent(agent, env);
        
        System.out.println("\n==========================================");
        System.out.println("            DQNè®­ç»ƒå®Œæˆ!                  ");
        System.out.println("==========================================");
    }

    /**
     * å±•ç¤ºé—®é¢˜èƒŒæ™¯
     */
    private static void demonstrateProblem() {
        System.out.println("ã€é—®é¢˜èƒŒæ™¯: CartPoleå€’ç«‹æ‘†ã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        System.out.println("ç›®æ ‡: é€šè¿‡å·¦å³ç§»åŠ¨å°è½¦,ä¿æŒæ†å­ç«–ç›´å¹³è¡¡");
        System.out.println();
        System.out.println("           |");
        System.out.println("           |  â† æ†å­");
        System.out.println("           |");
        System.out.println("      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”");
        System.out.println("      â”‚   å°è½¦   â”‚ â† å¯å·¦å³ç§»åŠ¨");
        System.out.println("      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        System.out.println("  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
        
        System.out.println("çŠ¶æ€ç©ºé—´(4ç»´è¿ç»­):");
        System.out.println("  â€¢ å°è½¦ä½ç½®: [-2.4, 2.4]");
        System.out.println("  â€¢ å°è½¦é€Ÿåº¦: [-âˆ, +âˆ]");
        System.out.println("  â€¢ æ†çš„è§’åº¦: [-0.21, 0.21] å¼§åº¦");
        System.out.println("  â€¢ æ†çš„è§’é€Ÿåº¦: [-âˆ, +âˆ]");
        
        System.out.println("\nåŠ¨ä½œç©ºé—´(2ç»´ç¦»æ•£):");
        System.out.println("  â€¢ åŠ¨ä½œ0: å‘å·¦æ¨ â†");
        System.out.println("  â€¢ åŠ¨ä½œ1: å‘å³æ¨ â†’");
        
        System.out.println("\nå¥–åŠ±è®¾è®¡:");
        System.out.println("  â€¢ æ¯æ­¥ä¿æŒå¹³è¡¡: +1");
        System.out.println("  â€¢ æ†å€’ä¸‹æˆ–è¶…ç•Œ: å›åˆç»“æŸ");
        System.out.println();
    }

    /**
     * åˆ›å»ºå’Œé…ç½®DQNæ™ºèƒ½ä½“
     */
    private static DQNAgent setupDQNAgent() {
        System.out.println("ã€DQNæ™ºèƒ½ä½“é…ç½®ã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        
        int stateDim = 4;
        int actionDim = 2;
        int[] hiddenSizes = {128, 128};
        float learningRate = 0.001f;
        float epsilon = 1.0f;  // åˆå§‹æ¢ç´¢ç‡
        float gamma = 0.99f;   // æŠ˜æ‰£å› å­
        int batchSize = 64;
        int bufferSize = 10000;
        int targetUpdateFreq = 100;
        
        System.out.println("ç½‘ç»œç»“æ„:");
        System.out.println("  è¾“å…¥å±‚: " + stateDim + " (çŠ¶æ€ç»´åº¦)");
        System.out.println("  éšè—å±‚: " + hiddenSizes[0] + " â†’ " + hiddenSizes[1]);
        System.out.println("  è¾“å‡ºå±‚: " + actionDim + " (Qå€¼)");
        
        System.out.println("\nè¶…å‚æ•°:");
        System.out.println("  å­¦ä¹ ç‡: " + learningRate);
        System.out.println("  åˆå§‹æ¢ç´¢ç‡: " + epsilon);
        System.out.println("  æŠ˜æ‰£å› å­: " + gamma);
        System.out.println("  æ‰¹æ¬¡å¤§å°: " + batchSize);
        System.out.println("  ç»éªŒç¼“å†²åŒº: " + bufferSize);
        System.out.println("  ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡: " + targetUpdateFreq + "æ­¥");
        
        DQNAgent agent = new DQNAgent(
            "CartPole-DQN",
            stateDim,
            actionDim,
            hiddenSizes,
            learningRate,
            epsilon,
            gamma,
            batchSize,
            bufferSize,
            targetUpdateFreq
        );
        
        System.out.println("\nâœ“ DQNæ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ\n");
        return agent;
    }

    /**
     * è®­ç»ƒDQNæ™ºèƒ½ä½“
     */
    private static void trainAgent(DQNAgent agent, CartPoleEnvironment env) {
        System.out.println("ã€å¼€å§‹è®­ç»ƒã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        System.out.println("æ€»å›åˆæ•°: " + MAX_EPISODES);
        System.out.println("è¯„ä¼°é—´éš”: æ¯" + EVAL_INTERVAL + "å›åˆ\n");
        
        System.out.println("å›åˆ |  å¥–åŠ±  |   Îµ   | ç¼“å†²åŒº | Loss");
        System.out.println("-----|--------|-------|--------|-------");
        
        float[] recentRewards = new float[10];
        int rewardIndex = 0;
        
        for (int episode = 0; episode < MAX_EPISODES; episode++) {
            Variable state = env.reset();
            float episodeReward = 0;
            int steps = 0;
            
            // å›åˆå¾ªç¯
            while (!env.isDone() && steps < 500) {
                // é€‰æ‹©åŠ¨ä½œ
                Variable action = agent.selectAction(state);
                
                // æ‰§è¡ŒåŠ¨ä½œ
                Environment.StepResult result = env.step(action);
                
                // å­˜å‚¨ç»éªŒå¹¶å­¦ä¹ 
                Experience experience = new Experience(
                    state, action, result.getReward(),
                    result.getNextState(), result.isDone(), steps
                );
                agent.learn(experience);
                
                episodeReward += result.getReward();
                state = result.getNextState();
                steps++;
            }
            
            // æ¢ç´¢ç‡è¡°å‡
            agent.decayEpsilon(0.995f);
            
            // è®°å½•æœ€è¿‘å¥–åŠ±
            recentRewards[rewardIndex] = episodeReward;
            rewardIndex = (rewardIndex + 1) % recentRewards.length;
            
            // å®šæœŸæ‰“å°è®­ç»ƒä¿¡æ¯
            if ((episode + 1) % 10 == 0) {
                System.out.printf(" %3d | %6.1f | %.3f | %5.1f%% | %.4f\n",
                    episode + 1,
                    episodeReward,
                    agent.getCurrentEpsilon(),
                    agent.getBufferUsage() * 100,
                    agent.getAverageLoss()
                );
            }
            
            // å®šæœŸè¯„ä¼°
            if ((episode + 1) % EVAL_INTERVAL == 0) {
                float avgReward = evaluatePerformance(agent, env, EVAL_EPISODES);
                System.out.println("â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€");
                System.out.printf("è¯„ä¼° | %6.1f | (å½“å‰æ€§èƒ½è¯„ä¼°)\n", avgReward);
                System.out.println("â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€");
                
                // åˆ¤æ–­æ˜¯å¦å·²ç»å­¦ä¼š
                if (avgReward >= 195) {
                    System.out.println("\nâœ“ æ™ºèƒ½ä½“å·²å­¦ä¼šæ§åˆ¶å€’ç«‹æ‘†!(å¹³å‡å¥–åŠ±â‰¥195)");
                    System.out.println("  åœ¨ç¬¬ " + (episode + 1) + " å›åˆè¾¾æˆç›®æ ‡");
                    break;
                }
            }
        }
        
        System.out.println();
    }

    /**
     * è¯„ä¼°æ€§èƒ½
     */
    private static float evaluatePerformance(DQNAgent agent, CartPoleEnvironment env, int episodes) {
        agent.setTraining(false);
        float totalReward = 0;
        
        for (int episode = 0; episode < episodes; episode++) {
            Variable state = env.reset();
            float episodeReward = 0;
            int steps = 0;
            
            while (!env.isDone() && steps < 500) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = env.step(action);
                
                state = result.getNextState();
                episodeReward += result.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
        }
        
        agent.setTraining(true);
        return totalReward / episodes;
    }

    /**
     * æœ€ç»ˆè¯„ä¼°
     */
    private static void evaluateAgent(DQNAgent agent, CartPoleEnvironment env) {
        System.out.println("ã€æœ€ç»ˆè¯„ä¼°ã€‘");
        System.out.println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        System.out.println("æµ‹è¯•" + EVAL_EPISODES + "ä¸ªå›åˆ,ä½¿ç”¨è´ªå¿ƒç­–ç•¥(ä¸æ¢ç´¢)\n");
        
        agent.setTraining(false);
        float totalReward = 0;
        int successCount = 0;
        
        System.out.println("å›åˆ | æ­¥æ•° | å¥–åŠ± | ç»“æœ");
        System.out.println("-----|------|------|--------");
        
        for (int episode = 0; episode < EVAL_EPISODES; episode++) {
            Variable state = env.reset();
            float episodeReward = 0;
            int steps = 0;
            
            while (!env.isDone() && steps < 500) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = env.step(action);
                
                state = result.getNextState();
                episodeReward += result.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
            if (episodeReward >= 195) {
                successCount++;
            }
            
            System.out.printf(" %2d  | %3d  | %4.0f | %s\n",
                episode + 1,
                steps,
                episodeReward,
                episodeReward >= 195 ? "æˆåŠŸâœ“" : "å¤±è´¥âœ—"
            );
        }
        
        float avgReward = totalReward / EVAL_EPISODES;
        float successRate = (float) successCount / EVAL_EPISODES * 100;
        
        System.out.println("\nè¯„ä¼°ç»“æœ:");
        System.out.println("  å¹³å‡å¥–åŠ±: " + String.format("%.2f", avgReward));
        System.out.println("  æˆåŠŸç‡: " + String.format("%.1f%% (%d/%d)", 
            successRate, successCount, EVAL_EPISODES));
        System.out.println("  æœ€å¤§å¯èƒ½å¥–åŠ±: 500");
        
        if (avgReward >= 195) {
            System.out.println("\nğŸ‰ æ­å–œ!DQNæˆåŠŸå­¦ä¼šæ§åˆ¶å€’ç«‹æ‘†!");
        } else if (avgReward >= 100) {
            System.out.println("\nğŸ‘ è¡¨ç°ä¸é”™,ä½†è¿˜æœ‰æå‡ç©ºé—´");
        } else {
            System.out.println("\nğŸ’ª ç»§ç»­è®­ç»ƒå¯èƒ½ä¼šæ›´å¥½");
        }
        
        System.out.println("\nã€DQNå…³é”®è¦ç‚¹ã€‘");
        System.out.println("âœ“ ç»éªŒå›æ”¾: æ‰“ç ´æ•°æ®ç›¸å…³æ€§,æé«˜æ ·æœ¬æ•ˆç‡");
        System.out.println("âœ“ ç›®æ ‡ç½‘ç»œ: ç¨³å®šè®­ç»ƒè¿‡ç¨‹,é˜²æ­¢éœ‡è¡");
        System.out.println("âœ“ Îµ-è´ªå¿ƒ: å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨,é€æ­¥æ”¶æ•›");
        System.out.println("âœ“ ç¥ç»ç½‘ç»œ: è¿‘ä¼¼Qå‡½æ•°,å¤„ç†é«˜ç»´çŠ¶æ€");
    }
}
